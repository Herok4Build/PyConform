#! /usr/bin/env python
"""
createOutputSpecs 

Creates the Json output specification file.

COPYRIGHT: 2016, University Corporation for Atmospheric Research
LICENSE: See the LICENSE.rst file for details
"""

import argparse, os, sys
import json
import miptableparser 
from dateutil.parser import parse
import datetime
from dreqPy import dreq

# Map netcdf types to python types
data_types = {'char': 'char', 'byte': 'int8', 'short': 'int16', 'int': 'int32',
              'float': 'float32', 'real': 'float32', 'double': 'float64', 
              'character':'char', 'integer':'int32'}

# The way the date should be formatted in the filenames
date_strings = {'yr': '__YYYY-YYYY__', 'mon': '__YYYYMM-YYYYMM__', 'monClim': '__YYYYMM-YYYYMM-clim__', 
                'day': '__YYYYMMDD-YYYYMMDD__', '6hr': '__YYYYMMDDHHMM-YYYYMMDDHHMM__', 
                '3hr': '__YYYYMMDDHHMM-YYYYMMDDHHMM__', '1hr': '__YYYYMMDDHHMM-YYYYMMDDHHMM__',
                'subhr': '__YYYYMMDDHHMM-YYYYMMDDHHMM__', '1hrClimMon': '__YYYYMMDDHHMM-YYYYMMDDHHMM-clim__'}

def parseArgs(argv = None):

    desc = "This tool creates a specification file that is needed to run PyConform."

    parser = argparse.ArgumentParser(prog='createOutputSpecs',
                                     description=desc)
    parser.add_argument('-d', '--defFile', default=None, type=str,
                        help='A file listing the variable definitions.', required=True)
    parser.add_argument('-g', '--globalAttrFile', default=None, type=str,
                        help='A file listing the global attributes that ' 
                             'are common to all files.')
    parser.add_argument('-e', '--exp', default='None', type=str,
                        help='The name of the experiment.')
    parser.add_argument('-m', '--mip', default='None', type=str,
                        help='The name of the MIPs to generate spec files for')
    parser.add_argument('-t', '--miptables',default='None', type=str,
                        help='The name of the MIP Tables to generate spec files for')
    parser.add_argument('-tt', '--mipTableType', default='xml', type=str,
                        help='MIP table file type.  Can be xml, cmor, or excel.')
    parser.add_argument('-u', '--userList', default=None, type=str,
                        help='A file containing cf-compliant names to derive.')
    parser.add_argument('-o', '--outputpath', default=os.getcwd(), type=str,
                        help='Output pathname for the output specification file(s).')

    return parser.parse_args(argv)


def load(defs):

    # Load the definition file - translations from  model variables to requested variables 

    def_dict = {}
    for line in defs:
        line = line.strip()
        # hanle comments
        if '#' in line:
            line = line.split('#')[0].strip()
        # slit definition into the two parts
        split = line.split('=')
        if (len(split) == 2):
            def_dict[split[0].strip()] = split[1].strip()
        else:
            if len(line)>0 :
                print 'Could not parse this line: ',line
    return def_dict

def defineVar(v, varName, attributes, definition, experiment):

    # Get variables needed to piece together the filename
    ripf_list = ['realization_index','initialization_index','physics_index','forcing_index']
    if all (ripf in attributes for ripf in ripf_list):
        ripf = ("r{0}i{1}p{2}f{3}".format(attributes['realization_index'],
                                                     attributes['initialization_index'],
                                                     attributes['physics_index'],
                                                     attributes['forcing_index']))
    else:
        ripf = ''

    mip_era = attributes['mip_era']
    activity_id = attributes['activity_id']
    institution_id = attributes['institution_id']
    source_id = attributes['source_id']
    grid = attributes['grid_label']
    version = attributes['version']
    sub_experiment_id = attributes['sub_experiment_id']
    #today = datetime.datetime.now()
    #version = 'v'+str(today.year).zfill(4)+str(today.month).zfill(2)+str(today.day).zfill(2)
    # modify and add to the global attributes that are requested
    v2 = dict(v)
    for key,value in v.iteritems(): # remove all attributes that do not have values
        if value == '':
            v2.pop(key,None)

    attributes2 = dict(attributes)

    attributes2["variable_id"] = v["variable_id"]
    if "realm" in v.keys():
        attributes2["realm"] = v["realm"]
    attributes2["creation_date"]=""
    attributes2.pop('version',None)

    f_format = attributes2["netcdf_type"]
    valid_formats = ['NETCDF4','NETCDF4_CLASSIC','NETCDF3_CLASSIC','NETCDF3_64BIT_OFFSET','NETCDF3_64BIT_DATA']
    if f_format not in valid_formats:
       print 'ERROR: ', f_format, ' is not a valid format.  Please choose from ',valid_formats
       sys.exit(-9) 
    attributes2.pop('netcdf_type',None)
   
    if 'NETCDF4' in f_format: 
        if 'compression' in attributes2.keys():
            compression = attributes2['compression'] 
            attributes2.pop('compression',None)
        else:
            compression = None

        if 'shuffle' in attributes2.keys():
            shuffle = attributes2['shuffle']
            attributes2.pop('shuffle',None)
        else:
            shuffle = None 
    else:
        compression = None   
        shuffle = None

    # Put together the filename
    mipTable = v['mipTable']
    if v["frequency"] in date_strings.keys():
        dst = date_strings[v["frequency"]]
    else:
        dst = ''
    f_name = ("{0}/{1}/{2}/{3}/{4}/{5}/{6}/{7}/{8}/{9}/{10}_{11}_{12}_{13}_{14}_{15}{16}.nc".format(
               mip_era, activity_id, institution_id, source_id, experiment, ripf, mipTable,
               varName, grid, version,
               varName, mipTable, source_id, experiment, ripf, grid, dst))

    var = {}
    # handle things that still aren't set correctly in the request
#    for k1,v1 in v.iteritems():
#        if not isinstance(v1,(list,str,float)):
#            v[k1] = "None"

    # create proper url for the further_info_url global attribute
    info_url = "{0}.{1}.{2}.{3}.{4}.{5}".format(mip_era, institution_id, source_id, experiment, sub_experiment_id, ripf) 
    attributes2['further_info_url'] = attributes2['further_info_url'] + info_url

    # put together the dictionary entry for this variable    
    var["attributes"] = v2
    var["definition"] = definition
    var["file"] = {}
    var["file"]["attributes"] = attributes2
    var["file"]["attributes"]["variant_label"] = ripf
    var["file"]["filename"] = f_name
    var["file"]["format"] = f_format 
    var["file"]["metavars"] = []
    if compression is not None:
        var["file"]["compression"] = compression
    if shuffle is not None:
        var["file"]["shuffle"] = shuffle

    if 'type' in v.keys() and v['type'] != 'None' and v['type'] != '' and v['type'] != None:
        var["datatype"]  = data_types[v['type']]
    else:
        var["datatype"] = 'None'
    if 'requested' in v.keys():
        if v['requested'] != '':
            var['definition'] =  v['requested']
    if 'coordinates' in v.keys():
        var["dimensions"] = v['coordinates'].split('|')

    # return the variable dictionary with all pieces added
    return var

def defineAxes(v, name):

    # define the axes that is used by at least one of the variables in the file

    var = {}
    v2 = dict(v)

    # remove all of the attributes that have no values    
    for key,value in v.iteritems():
        if value == '':
            v2.pop(key,None)

    # put everything into a variable dictionary
    var["attributes"] = v2
    if 'type' in v.keys():
        var["datatype"]  = data_types[v['type']]
    if 'requested' in v.keys():
        if v['requested'] != '':
            try:
                req = [float(val) for val in v['requested'].split()]
            except ValueError:
                req = v['requested'].split()
            except AttributeError:
                req = v['requested']
            if len(req) > 0:
                var['definition'] = req
            #var['definition'] =  v['requested'] 
    var['dimensions'] = [name]

    # return the variable dictionary with all pieces added
    return var


def getUserVars(fn):

    # create variables from a list supplied by the user

    variables = []
    with open(fn) as f:
        for vr in f:
            vr = vr.strip()
            if vr != "":
                variables.append(vr) 

    return variables 


def create_output(exp_dict, definitions, attributes, output_path, args, experiment):

    # create the output json files

    TableSpec = {}
    AllMissing = {}

    # go through each one of the data requests from the experiments
    for t,table_dict in exp_dict.iteritems():

        ReqSpec = {}
        variables = {}
        axes = {}
        table_info = {}

        AllMissing[t] = []

        # separate the data request
        variables = table_dict['variables']
        axes = table_dict['axes']
        table_info = table_dict['table_info']
        attributes.update(table_info)
        if 'generic_levels' in table_info.keys():
            g_levels = table_info['generic_levels']
            g_split = g_levels.split(' ')
            for l in g_split:
                axes[l] = {}

        identifier = t

        var_list = {}

        # Add axes into the variable list
        for v,d in axes.iteritems():
            var_list[v] = defineAxes(d, v)
            if v in definitions.keys() and 'definition' not in var_list[v].keys():
                var_list[v]['definition'] = definitions[v]


        # For each variable in the definition file, create a file entry in the spec and define it
        for v,d in variables.iteritems():
            if v in definitions.keys():
                var_list[v] = defineVar(d, v, attributes, definitions[v], experiment)
                realm = d["realm"].replace(' ','_')
                ts_key = var_list[v]["file"]["attributes"]["activity_id"]+'_'+var_list[v]["attributes"]["mipTable"]+'_'+realm
                if ts_key not in TableSpec.keys():
                    TableSpec[ts_key] = {}
                TableSpec[ts_key][v] = var_list[v]
                for dim in var_list[v]["dimensions"]:
                    if dim not in TableSpec[ts_key].keys() and dim != '' and dim != 'None':
                        TableSpec[ts_key][dim] = var_list[dim] 
            else:
                AllMissing[t].append(v)

        ReqSpec["variables"] = var_list

        # Write the JSON request spec file
        f = output_path + '/' +experiment + '/byRequest/' + experiment + '_' + identifier + '_spec.json'
        if not os.path.exists(output_path+'/' +experiment + '/byRequest/'):
            os.makedirs(output_path + '/' +experiment + '/byRequest/')
        with open(f, 'w') as outfile:
            json.dump(ReqSpec["variables"], outfile, sort_keys=True, indent=4)
        

    # create json files per MIP+table
    if not os.path.exists(output_path+'/' +experiment + '/byTable/'):
        os.makedirs(output_path + '/' +experiment + '/byTable/')
    for n,t in TableSpec.iteritems():
        spec = {}
        f = output_path + '/' +experiment + '/byTable/' + experiment + '_' + n + '_spec.json'
        spec["variables"] = t
        with open(f, 'w') as outfile:
            json.dump(t, outfile, sort_keys=True, indent=4)

    f1 = output_path + '/' +experiment + '/byTable/MISSING_DEFS.json'
    with open(f1, 'w') as outfile:
        json.dump(AllMissing, outfile, sort_keys=True, indent=4)
    f2 = output_path + '/' +experiment + '/byRequest/MISSING_DEFS.json'
    with open(f2, 'w') as outfile:
        json.dump(AllMissing, outfile, sort_keys=True, indent=4)

def create_non_mip_output(variables, definitions, outputpath):

    vs = {}
    missing = []
    spec = {}
    if 'file_suffix' in definitions.keys():
        fn_prefix = definitions['file_suffix']
    else:
        fn_prefix = ''
    if 'output_path' in definitions.keys():
        output_path = definitions['output_path']
    else:
        output_path = os.getcwd()

    for v in variables:
        var = v.split(':')
        vn = var[0]
        if vn in definitions.keys():
            vs[vn] = {}
            vs[vn]['definition'] = definitions[vn]
            if len(var) > 1:
                vs[vn]['dimensions'] = var[1].split('.')
            vs[vn]['filename'] = output_path+'/'+vn+'_'+fn_prefix+'.nc' 
            # get the dimensions
            for d in vs[vn]['dimensions']:
                if d not in vs.keys():
                    vs[d] = {}
                    vs[d]['definition'] = definitions[d] 
                    vs[d]['dimensions'] = d    
        else:
            missing.append(vn)
    spec["variables"] = vs
    spec["variables_missing_defs"] = missing

    # Write output json file
    if not os.path.exists(outputpath):
        os.makedirs(outputpath)           
 
    f = outputpath + '/user_defined.json' 
    with open(f, 'w') as outfile:
        json.dump(spec, outfile, sort_keys=True, indent=4)


def main(argv=None):
   
    args = parseArgs(argv)
    
    print "\n" 
    print "------------------------------------------"
    print 'Running createOutputSpecs with these args:\n'
    print 'Variable Definitions: ', args.defFile
    print 'Global Attributes to be added to each file: ', args.globalAttrFile
    print 'Experiment Name: ', args.exp
    print 'MIPs: ', args.mip
    print 'MIP Tables: ', args.miptables
    print 'MIP Table Type: ',args.mipTableType
    print 'User supplied variable list: ',args.userList
    print 'Will create output spec files within this directory:', args.outputpath
    print "------------------------------------------"

    # Open/Read the definition file
    if os.path.isfile(args.defFile):
        with open(args.defFile) as y_definitions:
            definitions = load(y_definitions)
            #print 'DEFINITIONS: ',definitions
    else:
        print 'Definition file does not exist: ',args.defFile
        os.sys.exit(1)

    # Open/Read the global attributes file
    attributes = {}
    if args.globalAttrFile and os.path.isfile(args.globalAttrFile):
        with open(args.globalAttrFile) as y_attributes:
            attributes = load(y_attributes)
            #print 'GLOBAL ATTRIBUTES: ',attributes
    else:
        if args.globalAttrFile and not os.path.isfile(args.globalAttrFile):    
            print 'Global Attributes file does not exist: ',args.globalAttrFile
            os.sys.exit(1)

    # Open/Read the MIP table
    #if args.mipTableType != None and args.exp != None:
    exps = args.exp.split(',')
    if exps[0] == 'None':
        exps = ['--ALL--']
    mips = args.mip.split(',')
    if mips[0] == 'None':
        mips = ['--ALL--']
    tables = args.miptables.split(',')
    if tables[0] == 'None':
        tables = ['--ALL--']
   
    dq = dreq.loadDreq()
    if '--ALL--' in exps:
        exps = dq.inx.experiment.label.keys()

    # Go through a user supplied list if specified
    variables = []
    if args.userList and os.path.isfile(args.userList):
        variables = getUserVars(args.userList)
    else:
        if args.userList and not os.path.isfile(args.userList):
            variables = args.userList.split(',')

    if 'None' in args.mipTableType.capitalize():
        create_non_mip_output(variables, definitions, args.outputpath)
    else:
        for exp in exps: 
            exp_dict = miptableparser.mip_table_parser(exp, mips, tables, variables, type=args.mipTableType)

            # Write the spec files out to disk
            create_output(exp_dict, definitions, attributes, args.outputpath, args, exp)

if __name__ == '__main__':
    main()
